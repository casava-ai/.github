# Casava 
  Advanced Infrastructure Consulting

## Goals

:cyclone: Build scalable AI applications faster. 
:shipit: Lower cost, lower latency, more secure. 
:godmode: Cloud Agnostic


## Features 
``` 
‚è© instant infrastructure setup and tear-down 
üåÇ local or cloud-agnostic 
üìà monitoring dashboard 
üå≤ power and memory-efficient application deployment  
üßë‚ÄçüöÄ Advanced LLM based virtual operating system 
```
## Ethos

At Casava, we are determined to bring the latest advancements and best practices of owning your AI infrastructure to as many people as possible.

The last few months have felt like the beginning of humanity's conversation with AI. Although prior architectures allowed us to achieve sub-verbal communication with neural networks, the emergence of GPT-3 turned this verbal, greatly increasing the bandwidth of AI-human conversation and potential applications.

Building a well-designed codebase that backs an intelligent application breaks down to three stages; 
1. efficient inference infrastructure
2. model fine-tuning and prompting
3. compilation and interpretation of output

This third stage can be viewed as the exact sort of problem traditional programming-language to machine-language compilers have been designed to solve efficiently, and much of the theory involved carries over when thinking about how to write software that compiles natural language to LLM input, or LLM output to a data structure. We believe that building successful new LLM software will depend on how well we can build the necessary compilers to translate between
LLMs and our software, and vice versa.

```
From a business standpoint, it's not clear to us yet where the value will accrue.
Will it be the underlying APIs, or the apps built on top of them?
The best models or the best model serving infrastructure?
```

This quote from Ankur Goyal elegantly summarises some of the challenges this technology will present to us. This nature of interplay between optimizing model output versus building out more advanced, smarter infrastructure that surrounds the models is one of the core conundrums we tend to wrestle with when thinking about how to proceed best when working with LLMs, and may even strike at the heart of the larger questions around AI alignment.
